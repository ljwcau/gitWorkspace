# 房价预测资料收集

1. 用pandas库读取csv文件，评估指标选用PR曲线的AUC，Windows的scikit-learn库搭建。https://github.com/9468305/python-script/tree/master/auc_pr_roc
---
## 数学建模中用于数据预测的模型

|预测模型名称 |适用范围|优点|缺点|
|:------:|:------|:------|:------|
|灰色预测模型|采用非原始数据，而是生成的数据序列|可解决历史数据少，可靠性低的问题|只适用于中短期的预测，适合近似指数增长的预测|
|插值和拟合|适用于有运动轨迹图像的模型|曲面、曲线拟合，找出一种方法使得仿真曲线面最大程度的接近原来的|
|时间序列预测法|运用过去的历史数据，进一步推测市场未来的发展趋势|一般用ARMA模型拟合时间序列，预测该时间序列未来值。Daniel检验平稳性|外界较大变化时预测效果偏差大，中短期预测比长期好|
|马尔科夫预测|适用于随机现象、已知现情况，系统未来时刻只与现在有关，而与过去无直接关系|现时刻累计销售额已知，研究商店的未来某一时刻的销售额|不适宜系统中长期预测|
|差分方程|利用差分方程建模研究实际问题，常需根据统计数据用最小二乘法来拟合出差分方程的系数|适用于商品销售量的预测、投资保险收益率的预测。|稳定性还要讨论代数方程的求根|
|微分方程模型|适用于基于相关原理的因果预测模型，大多是物理或几何方面的典型问题，假设条件，用数学符号表示规律，列出方程，求解的结果就是问题的答案|短、中、长期的预测都适合，传染病经济增长（或人口）、Lanchester战争|反应事物内部规律及其内在关系，但由于方程的建立是以局部规律的独立性假定为基础，当作为长期预测时，误差较大，且微分方程的解比较难以得到。|
|神经元网络|常用BP神经网络和径向基函数神经网络|BP神经网络拓扑结构及其训练模式。RBF神经网络结构及其学习算法|模型案例：预测某水库的年径流量和因子特征值|

## 机器学习中关于预测的模型

    一个思考：是不是有监督的学习，都能进行预测，例如输入x得到y=f(x)


机器学习新手必看十大算法 | 机器之心  https://www.jiqizhixin.com/articles/a-tour-of-the-top-10-algorithms-for-machine-learning-newbieshttps://www.jiqizhixin.com/articles/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies

|机器学习算法名称|算法简介|
|-------|------|
|线性回归|例如：y = B0 + B1 * x我们将根据输入 x 预测 y，线性回归学习算法的目标是找到系数 B0 和 B1 的值。可以使用不同的技术从数据中学习线性回归模型，例如用于普通最小二乘法和梯度下降优化的线性代数解|
|Logistic回归||只能分两类，logistic 函数看起来像一个大的 S，并且可以将任何值转换到 0 到 1 的区间内我们可以规定 logistic 函数的输出值是 0 和 1（例如，输入小于 0.5 则输出为 1）并预测类别值|
|线性判别分析（LDA）|可以进行多分类，LDA 由数据的统计属性构成，对每个类别进行计算。单个输入变量的 LDA 包括：每个类别的平均值、所有类别的方差。进行预测的方法是计算每个类别的判别值并对具备最大值的类别进行预测。该技术假设数据呈高斯分布（钟形曲线），因此最好预先从数据中删除异常值。|
|决策树|决策树的叶节点包含一个用于预测的输出变量 y。通过遍历该树的分割点，直到到达一个叶节点并输出该节点的类别值就可以作出预测。决策树学习速度和预测速度都很快。它们还可以解决大量问题，并且不需要对数据做特别准备|
|朴素贝叶斯|朴素贝叶斯之所以是朴素的，是因为它假设每个输入变量是独立的。该模型由两种概率组成，这两种概率都可以直接从训练数据中计算出来：1）每个类别的概率；2）给定每个 x 的值，每个类别的条件概率。一旦计算出来，概率模型可用于使用贝叶斯定理对新数据进行预测。当你的数据是实值时，通常假设一个高斯分布（钟形曲线），这样你可以简单的估计这些概率。|
|K 近邻算法（KNN）|KNN 算法在整个训练集中搜索 K 个最相似实例（近邻）并汇总这 K 个实例的输出变量，以预测新数据点。对于回归问题，这可能是平均输出变量，对于分类问题，这可能是众数（或最常见的）类别值。诀窍在于如何确定数据实例间的相似性，KNN 需要大量内存或空间来存储所有数据，但是只有在需要预测时才执行计算（或学习），需要选择最相关的属性，避免维数灾难|
|学习向量量化算法（简称 LVQ）|一种人工神经网络算法，它允许你选择训练实例的数量，并精确地学习这些实例应该是什么样的。LVQ 的表示是*码本向量*的集合。这些是在开始时随机选择的，并逐渐调整以在学习算法的多次迭代中最好地总结训练数据集。在学习之后，码本向量可用于预测（类似 K 近邻算法）。最相似的近邻（最佳匹配的码本向量）通过计算每个码本向量和新数据实例之间的距离找到。然后返回最佳匹配单元的类别值或（回归中的实际值）作为预测。如果你重新调整数据，使其具有相同的范围（比如 0 到 1 之间），就可以获得最佳结果。**如果你发现 KNN 在你的数据集上达到很好的结果，请尝试用 LVQ 减少存储整个训练数据集的内存要求。**|
|支持向量机|超平面和最近的数据点之间的距离被称为间隔。分开两个类别的最好的或最理想的超平面具备最大间隔。只有这些点与定义超平面和构建分类器有关。这些点被称为支持向量，它们支持或定义了超平面。实际上，优化算法用于寻找最大化间隔的系数的值|
|随机森林|是 Bootstrap Aggregation（又称 bagging）集成机器学习算法，在随机森林的方法中决策树被创建以便于通过引入随机性来进行次优分割，而不是选择最佳分割点。**如果你用方差较高的算法（如决策树）得到了很好的结果，那么通常可以通过 bagging 该算法来获得更好的结果。**|
|AdaBoost |AdaBoost 是第一个为二分类开发的真正成功的 boosting 算法， boosting 方法建立在 AdaBoost 之上，最显著的是随机梯度提升，AdaBoost 与短决策树一起使用。在第一个决策树创建之后，利用每个训练实例上树的性能来衡量下一个决策树应该对每个训练实例付出多少注意力。难以预测的训练数据被分配更多权重，而容易预测的数据分配的权重较少。依次创建模型，每个模型在训练实例上更新权重，影响序列中下一个决策树的学习。在所有决策树建立之后，对新数据进行预测，并且通过每个决策树在训练数据上的精确度评估其性能。**具备已删除异常值的干净数据非常重要**|

## 还需复习

1. 二叉树、完全树的概念
2. 计算复杂性
3. 回归问题与分类问题的区别！
4. 强化学习

        强化学习是一个有趣的学习模型，不仅能学习如何将输入映射到输出，还能学习如何借助依赖关系将一系列输入映射到输出（例如 Markov 决策流程）。在环境中的状态和给定状态下的可能操作的上下文中，可以应用强化学习。在学习过程中，该算法随机探索某个环境中的状态-操作对（以构建一个状态-操作对表），然后应用所学信息来挖掘状态-操作对奖励，以便为给定状态选择能导致某个目标状态的最佳操作。要进一步了解强化学习。Q-learning 算法