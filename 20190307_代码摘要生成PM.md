# 文献综述

生成代码的文本摘要（1）_Zhiheng_Zhang_新浪博客  http://blog.sina.com.cn/s/blog_1a1a864d20102ybyd.html
（以下文本均转载自上述网站，感谢zhang同学的整理！）
## 代码生成摘要

该课题的当代进展，按照发展顺序排列：

1. IR检索技术  
该技术利用词性标注识别出最有可能体现代码特性的关键词;然后分析修正在词性标注过程中可能引入的错误;其次,对标识出的关键词进行降噪,以减少文本噪声带来的不利影响;最后,从关键词中选取若干个权值最高的词以组成代码摘要。但是面临着关键词抽取困难等弊端。
文献：Automatic generation of natural language summaries for java classes.
文献：Source code summarization technology based on syntactic analysis
引入代码克隆检测技术检测相似片段，提取相似片段的注释信息。
文献：Clocom: Mining existing source code for automatic comment generation.

|建立源代码的语言模型：|文献|
|------|------|
|fault detection|文献：On the naturalness of buggy code|
|code completion|文献：A statistical semantic language model for source code|
|code summarization|文献：Summarizing source code using a neural attention model.|

2. 基于深度学习&结合Attention机制

>Attention机制的起源于计算机视觉  
文献：Recurrent Models of Visual Attention  
首次将Attention机制用于NLP  
文献：Neural Machine Translation by Jointly Learning to Align and Translate
Goolge团队于2017引入了自注意力机制  
文献：Attention is All You Need  
将Attention机制用于生成代码摘要

- 深度学习模型：CODE-NN Model  
使用LSTM & Attention模型进行从源代码到文本摘要的过渡。

- 基于卷积的注意力机制模型：CBAM  
    文献：A Convolutional Attention Network for Extreme Summarization of Source Code
    文献：CBAM: Convolutional Block Attention Module

>神经网络中的注意力机制可以用来处理输入输出不固定维数的问题，其中包含着局部平移不变的特征，但是原来的体系结构并不是用来学习这些特征的。对方法进行优化：增加对输入符号的卷积操作，去提取这些特征，应用在代码生成的问题。

- 利用API信息辅助建模：TL-CodeSum  
以上是将源代码当做纯文本符号来进行到摘要的转译，并没有将源代码的信息充分利用起来，我们使用API信息进行辅助建模

- 加入源代码的结构（Structure）信息辅助建模  
文献：Deep code comment generation

目前论文工作的思考：
1、如果以代码生成摘要为目标，可在原有基础上，结合谷歌最新的多头自注意力机制 “Multi-headed self-attention”，对传统的基于注意力的算法流程进行创新与改良。
2、上述工作建立在将代码视为”plain text”的基础上，可参考上述进展中的叙述:“建立源代码的语言模型”，对源代码的模型表示方法进行改革（如上述文献中加入API数据、结构信息作为辅助，或重新确立一个模型表示方法）。